{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarianaGCosta/Lematizacao-do-portugues/blob/main/Lematiza%C3%A7%C3%A3o_em_dados_do_portugu%C3%AAs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ZJqcrMnHq8"
      },
      "source": [
        "# Lematização em dados do português\n",
        "\n",
        "Esse notebook aborda o processamento sintático de textos em português brasileiro em Python 3.10 e abrange as etapas de importação, leitura, normalização, tokenização e lematização. O código foi desenvolvido como parte de um relatório para a disciplina de **Recuperação da Informação**, do curso de Ciência da Computação da Universidade Federal do Rio de Janeiro (UFRJ), ministrado pela professora Giseli Rabello em 2024 com apoio do monitor Henrique Fernandes Rodrigues. O dataset analisado pode ser encontrado em [D&G UFF](https://deg.uff.br/corpus-dg/).\n",
        "\n",
        "**Mariana Gonçalves da Costa** [Programa de Pós-Graduação em Informática/UFRJ]\n",
        "\n",
        "\n",
        "Last updated: 12 December 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "i5u5ShpKjGf5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyjpSvSMh8ai"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh6oh_2shH6Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAIoeFYx2EQT",
        "outputId": "dc180363-9e90-4c60-b2e6-27a235cc4496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Corpus DeG\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Corpus DeG\")\n",
        "\n",
        "print(os.getcwd())  # Imprime o diretório de trabalho atual"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/drive/MyDrive/Corpus DeG'"
      ],
      "metadata": {
        "id": "kyn-ynU2dNzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ach6bHiDh4PS"
      },
      "source": [
        "## Processamento do Texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gA5jQjzJSCK"
      },
      "outputs": [],
      "source": [
        "def read_files(file):\n",
        "  \"\"\" Lê o conteúdo de um arquivo .txt \"\"\"\n",
        "\n",
        "  with open(file, 'r', encoding='latin-1') as file:\n",
        "    file = file.read()\n",
        "\n",
        "    return file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLc69ZZc-BXr"
      },
      "outputs": [],
      "source": [
        "# Lê o conteúdo em 'rio_grande.txt'\n",
        "rio_grande = read_files('rio_grande.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwordsdict = {\n",
        "    \"artigos\": [\"a\", \"o\", \"as\", \"os\", \"um\", \"uns\", \"uma\", \"umas\"],\n",
        "    \"pronomes\": [\"eu\", \"tu\", \"ele\", \"ela\", \"nós\", \"vós\", \"eles\", \"elas\",\n",
        "                \"me\", \"te\", \"lhe\", \"o\", \"a\", \"se\", \"nos\", \"vos\", \"lhes\", \"os\", \"as\", \"se\",\n",
        "                \"meu\", \"minha\", \"teus\", \"tuas\", \"seu\", \"sua\", \"seus\", \"suas\", \"nosso\",\"nossa\",\n",
        "                \"nossos\", \"nossas\", \"dela\", \"dele\", \"deles\", \"delas\", \"que\", \"qual\", \"quais\", \"quem\",\n",
        "                \"cujo\", \"cuja\", \"cujos\", \"cujas\", \"este\", \"esta\", \"estes\", \"estas\",\n",
        "                \"esse\", \"essa\", \"esses\", \"essas\", \"aquele\", \"isto\", \"isso\", \"aquilo\",\n",
        "                \"aquela\", \"aqueles\", \"aquelas\"],\n",
        "    \"preposicoes\": [\"com\", \"sem\", \"para\", \"de\", \"do\", \"da\", \"dos\", \"das\", \"em\",\n",
        "                    \"no\", \"na\", \"nos\", \"nas\", \"num\" \"por\", \"que\", \"ao\", \"à\"],\n",
        "    \"adverbios\": [\"assim\", \"aqui\", \"lá\", \"cá\", \"já\", \"logo\", \"muito\", \"tão\",\n",
        "                  \"tanto\", \"quase\", \"sim\", \"não\"],\n",
        "    \"conjuncoes\": [\"e\", \"mas\", \"entretanto\", \"porém\", \"pois\", \"porque\", \"ora\", \"ou\", \"logo\",\n",
        "                  \"como\", \"seja\", \"todavia\", \"nem\", \"desde\"],\n",
        "    \"verbos\": [\"ser\", \"sido\", \"sendo\", \"sou\", \"és\", \"é\", \"somos\", \"sois\", \"são\",\n",
        "              \"era\", \"eras\", \"éramos\", \"éreis\", \"eram\",\n",
        "              \"fui\", \"foste\", \"foi\", \"fomos\", \"fostes\", \"foram\", \"fora\",\n",
        "              \"seja\", \"sejas\", \"sejamos\", \"sejais\", \"sejam\", \"fosse\", \"fôssemos\", \"fossem\",\n",
        "              \"for\", \"formos\", \"forem\",\n",
        "\n",
        "              \"estar\", \"estando\", \"está\", \"estado\"\n",
        "              \"estou\", \"estás\", \"estamos\", \"estais\", \"estão\",\n",
        "              \"estava\", \"estavas\", \"estávamos\", \"estáveis\", \"estavam\",\n",
        "              \"estive\", \"estiveste\", \"esteve\", \"estivemos\", \"estivestes\", \"estiveram\",\n",
        "              \"estivera\", \"estiveras\", \"estivéramos\", \"estivéreis\", \"estiveram\",\n",
        "              \"estarei\", \"estarás\", \"estará\", \"estaremos\", \"estareis\", \"estarão\",\n",
        "              \"estaria\", \"estarias\", \"estaríamos\", \"estaríeis\", \"estariam\",\n",
        "              \"esteja\", \"estejas\", \"estejamos\", \"estejais\", \"estejam\",\n",
        "              \"estivesse\", \"estivesses\", \"estivéssemos\", \"estivésseis\", \"estivessem\",\n",
        "              \"estiver\", \"estiveres\", \"estivermos\", \"estiverdes\", \"estiverem\"\n",
        "\n",
        "              \"ter\", \"tido\", \"tendo\",\n",
        "              \"tenho\", \"tens\", \"tem\", \"temos\", \"tendes\", \"têm\",\n",
        "              \"tinha\", \"tinhas\", \"tínhamos\", \"tínheis\", \"tinham\",\n",
        "              \"tive\", \"tiveste\", \"teve\", \"tivemos\", \"tivestes\", \"tiveram\",\n",
        "              \"tivera\", \"tiveras\", \"tivéramos\", \"tivéreis\", \"tiveram\",\n",
        "              \"terei\", \"terás\", \"terá\", \"teremos\", \"tereis\", \"terão\",\n",
        "              \"teria\", \"terias\", \"teríamos\", \"teríeis\", \"teriam\",\n",
        "              \"tenha\", \"tenhas\", \"tenha\", \"tenhamos\", \"tenhais\", \"tenham\",\n",
        "              \"tivesse\", \"tivesses\", \"tivéssemos\", \"tivésseis\", \"tivessem\",\n",
        "              \"tiver\", \"tiveres\", \"tivermos\", \"tiverdes\", \"tiverem\", \"tende\",\n",
        "\n",
        "              \"haver\", \"havido\", \"havendo\", \"hei\", \"hás\", \"há\", \"havemos\", \"haveis\", \"hão\",\n",
        "              \"havia\", \"havias\", \"havíamos\", \"havíeis\", \"haviam\",\n",
        "              \"houve\", \"houveste\", \"houvemos\", \"houvestes\", \"houveram\",\n",
        "              \"houvera\", \"houveras\", \"houvéramos\", \"houvéreis\",\n",
        "              \"haverei\", \"haverás\", \"haverá\", \"haveremos\", \"havereis\", \"haverão\",\n",
        "              \"haveria\", \"haverias\", \"haveríamos\", \"haveríeis\", \"haveriam\",\n",
        "              \"haja\", \"hajas\", \"hajamos\", \"hajais\", \"hajam\",\n",
        "              \"houvesse\", \"houvesses\", \"houvéssemos\", \"houvésseis\", \"houvessem\",\n",
        "              \"houver\", \"houveres\", \"houvermos\", \"houverdes\", \"houverem\"],\n",
        "    \"marcador\": [\"ai\", \"aí\", \"né\", \"i\", \"ih\", \"ah\", \"oh\", \"eh\"],\n",
        "    \"numeros\" : ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "    }\n",
        "\n",
        "# Sugestões de remoções: estado, estáveis, eras, tende.\n",
        "\n",
        "# I organized the stopwords into a dictionary, grouping them by word classes.\n",
        "# This structure allows you to select relevant stopwords for your data cleaning to align with the goals of text processing.\n",
        "\n",
        "# Organizei as stopwords em um dicionário, agrupando-as pela classe de palavras.\n",
        "# Essa estrutura permite selecionar stopwords relevantes para a sua limpeza de dados de acordo com os objetivos do processamento de texto."
      ],
      "metadata": {
        "id": "22YWKbX9bsyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"stopwords.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(stopwordsdict, json_file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "2McR-wwRlb6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_stopwords(stopwords, word_classes, remove_list = None):\n",
        "  \"\"\"Cria uma lista de stopwords selecionadas a partir de um dicionário.\n",
        "\n",
        "  Argumentos:\n",
        "    stopwordsdict (dict): Dicionário contendo todas as stopwords por classe de palavras\n",
        "\n",
        "    word_classes (list): A lista de classes de palavras a serem incluídas nas stopwords finais.\n",
        "    As classes podem ser:\n",
        "    artigos, pronomes, preposicoes, adverbios, conjuncoes, verbos, marcadores discursivos, números\n",
        "\n",
        "    remove_list (list, opcional): Uma lista com palavras a serem removidas das stopwords\n",
        "\n",
        "  Retorna:\n",
        "    stopwords (list): Uma lista final de stopwords.\n",
        "  \"\"\"\n",
        "\n",
        "  stopwords = [word for word_class in word_classes for word in stopwordsdict.get(word_class, [])]\n",
        "  # Use dict.get() para evitar KeyError caso a classe não se encontre no dicionário\n",
        "\n",
        "  # Remove palavras selecionadas da list de stopwords caso o argumento remove_list seja preenchido\n",
        "  # com uma lista de remoção\n",
        "\n",
        "  if remove_list:\n",
        "      stopwords = [word for word in stopwords if word not in remove_list]\n",
        "\n",
        "  return stopwords"
      ],
      "metadata": {
        "id": "LjoxpLKHlY_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = select_stopwords(stopwordsdict, [\"artigos\", \"pronomes\", \"preposicoes\", \"adverbios\", \"conjuncoes\", \"marcador\", \"numeros\"])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rdc7vTntlgPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pontuações e símbolos presentes nos arquivos de acordo com as diretrizes de\n",
        "# transcrição do DeG\n",
        "\n",
        "punctuation = \", . / \\ [ ] ( ) : ; ? ! � … - \\x94\""
      ],
      "metadata": {
        "id": "mNOnMXZhb7OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_blocos(text):\n",
        "  \"\"\"Separa os blocos de texto em listas para que sejam recuperáveis depois do\n",
        "  tratamento sintático.\n",
        "\n",
        "  Recebe: text (str): O texto em txt.\n",
        "\n",
        "  Retorna: Texto original (list): Uma lista com os blocos de texto\n",
        "  separados.\"\"\"\n",
        "\n",
        "  # Verifica se o texto já está dividido em blocos\n",
        "  if isinstance(text, list):\n",
        "    return text  # Se já for uma lista, retorna como está\n",
        "  else:\n",
        "    return text.split('\\n\\n')  # Se for string, divide em blocos"
      ],
      "metadata": {
        "id": "ngMFnkBldcAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rio_grande = split_blocos(rio_grande)\n",
        "rio_grande = [lst for lst in rio_grande if lst]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_1v6orWZeNvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# Cria uma deep copy do dataset para manter os originais\n",
        "riogrande = copy.deepcopy(rio_grande)"
      ],
      "metadata": {
        "id": "k12LZDFYfaXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "675ht4jkjg16"
      },
      "outputs": [],
      "source": [
        "def tokenization(text, punctuation):\n",
        "    \"\"\"Tokeniza as palavras em um texto a partir dos espaços em branco e dos separadores.\n",
        "\n",
        "    Recebe:\n",
        "        text (list): Lista de strings com o texto a ser tokenizado.\n",
        "        punctuation (str): String contendo os separadores presentes no texto.\n",
        "\n",
        "    Retorna:\n",
        "        list: Uma lista de listas com os tokens (palavras) presentes em cada bloco de texto.\n",
        "    \"\"\"\n",
        "    # Cria uma tabela de tradução para substituir os separadores por espaços\n",
        "    translation_table = str.maketrans(punctuation, \" \" * len(punctuation))\n",
        "\n",
        "    # Aplica a tradução e tokeniza cada linha\n",
        "    return [line.translate(translation_table).split() for line in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r3S6BzBjstx"
      },
      "outputs": [],
      "source": [
        "def preprocessing(text, punctuation):\n",
        "  \"\"\" Executa o pré-processamento de um texto.\n",
        "\n",
        "  Recebe: uma lista com o texto e uma lista de caracteres separadores.\n",
        "\n",
        "  Retorna: uma lista de palavras pré-processadas referentes ao texto passado.\n",
        "  \"\"\"\n",
        "\n",
        "  tokens = tokenization(text, punctuation)\n",
        "\n",
        "  tokens_normais = [[token.lower() for token in sublist] for sublist in tokens]\n",
        "\n",
        "  return tokens_normais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugpVLrgXHxZ7"
      },
      "outputs": [],
      "source": [
        "# print(\"Pré-processamento da Base Rio Grande:\")\n",
        "riogrande = preprocessing(rio_grande, punctuation)\n",
        "# print(riogrande)\n",
        "# print(len(riogrande))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "riogrande_sem_stopwords = [[word for word in sublist if word not in stopwords] for sublist in riogrande]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9dEdzA4hl2Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Eug5YvxnjEjp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR28sM_xNvdy"
      },
      "source": [
        "## Abordagens de lematização\n",
        "\n",
        "\n",
        "1.   **Lematização por aprendizado de máquina**: spaCy modelo pt_core_news_sm\n",
        "2.   **Lematização por depêndencias universais**: Simplemma UD-PT-GSD\n",
        "3.   **Lematização por arquivos lexicográficos**: PortiLexicon-UD (Projeto POeTiSA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação do modelo do português do spaCy"
      ],
      "metadata": {
        "id": "YZAQOaCFO0ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download pt_core_news_sm --quiet\n",
        "\n",
        "nlp = spacy.load('pt_core_news_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR9GW7YH8jeV",
        "outputId": "b992580a-6310-454d-e82f-f39e39108d8e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/13.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/13.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/13.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/13.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m11.3/13.0 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematização por modelo e UD: spaCy"
      ],
      "metadata": {
        "id": "kDTP-CN58gHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizationspacy(tokens):\n",
        "  \"\"\" Lematiza um texto usando a bibliotecaspaCy.\n",
        "\n",
        "  Recebe: uma lista de tokens (str).\n",
        "\n",
        "  Retorna: uma lista com os tokens lematizados.\n",
        "  \"\"\"\n",
        "  text = ' '.join(tokens)  # Une os tokens em uma única string\n",
        "  doc = nlp(text)\n",
        "\n",
        "  lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "  return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "9Oza3WMYcpPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DataFrame\n",
        "riograndelem = pd.DataFrame({'Texto': riogrande})\n",
        "\n",
        "# Aplicar lematização aos dados\n",
        "riograndelem['Texto Lematizado'] = riograndelem['Texto'].apply(lemmatizationspacy)"
      ],
      "metadata": {
        "id": "xZIy0Nfx5A0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_lem = [riograndelem[i] for i in range(len(riograndelem))]\n",
        "spacy_lem = [s.lower() for s in spacy_lem if isinstance(s, str)]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U-QHGBZN2lGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DataFrame\n",
        "riograndelem2 = pd.DataFrame({'Texto': riogrande_sem_stopwords})\n",
        "\n",
        "# Aplicar lematização aos dados\n",
        "riograndelem2['Texto Lematizado'] = riograndelem2['Texto'].apply(lemmatizationspacy)"
      ],
      "metadata": {
        "id": "y6x1XLUPmB05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporta os dataframes da lematização em um arquivo .csv\n",
        "riograndelem.to_csv(r\"lematizacao_spacyRI.csv\")\n",
        "riograndelem2.to_csv(r\"lematizacao_spacyRI_sem_stopwords.csv\")"
      ],
      "metadata": {
        "id": "RByQx4cygt55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematização com [Simplemma](https://github.com/adbar/simplemma)"
      ],
      "metadata": {
        "id": "c5Pes4KAGzME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/adbar/simplemma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YG28XP4BDXD",
        "outputId": "ffb005be-6d08-4798-9a4f-d3450df486ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/adbar/simplemma\n",
            "  Cloning https://github.com/adbar/simplemma to /tmp/pip-req-build-l891p2ou\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/adbar/simplemma /tmp/pip-req-build-l891p2ou\n",
            "  Resolved https://github.com/adbar/simplemma to commit be7435ece1093b64bab01214d0770f8a78e7baf5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: simplemma\n",
            "  Building wheel for simplemma (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplemma: filename=simplemma-1.1.2-py3-none-any.whl size=67154856 sha256=9518e240e5a3ba2905f3f268678477f5c420140324c3efeeaef003e3b12d1bb9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-70078r0y/wheels/f8/a6/06/b7cdede3f6ff48350da0e1d78f5c1f948067ad45f092e36949\n",
            "Successfully built simplemma\n",
            "Installing collected packages: simplemma\n",
            "Successfully installed simplemma-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import simplemma\n",
        "\n",
        "def simplemmatization(texto):\n",
        "  \"\"\" Lematiza um texto através da função token.lemma_ do spaCy.\n",
        "\n",
        "  Recebe: um texto (string).\n",
        "\n",
        "  Retorna: um dataframe com o texto original e o texto lematizado.\n",
        "  \"\"\"\n",
        "\n",
        "  lemmas = []\n",
        "  for token in simplemma.lemmatize(texto, lang = 'pt'):\n",
        "    lemmas.append(token)\n",
        "  return ''.join(lemmas)"
      ],
      "metadata": {
        "id": "hpzaDfm2BfCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DataFrame\n",
        "simlematizacao = pd.DataFrame({'Texto': [item for sublist in riogrande for item in sublist]})\n",
        "\n",
        "# Aplicar lematização aos dados\n",
        "simlematizacao['Texto Lematizado'] = simlematizacao['Texto'].apply(simplemmatization)"
      ],
      "metadata": {
        "id": "hCeFxUbk44Am",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DataFrame\n",
        "simlematizacao2 = pd.DataFrame({'Texto': [item for sublist in riogrande_sem_stopwords for item in sublist]})\n",
        "\n",
        "# Aplicar lematização aos dados\n",
        "simlematizacao2['Texto Lematizado'] = simlematizacao2['Texto'].apply(simplemmatization)"
      ],
      "metadata": {
        "id": "g-diAfmfmzsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_lematizacao = simlematizacao['Texto Lematizado'].values.tolist()"
      ],
      "metadata": {
        "id": "T5EhT30PKIaZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporta o dataframe da lematização em um arquivo .csv\n",
        "simlematizacao.to_csv(r\"simlematizacaoRI.csv\")\n",
        "simlematizacao2.to_csv(r\"simlematizacaoRI_sem_stopwords.csv\")"
      ],
      "metadata": {
        "id": "f5wwF2S1C3H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematização com PortiLexicon-UD"
      ],
      "metadata": {
        "id": "CvbAAwineYU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "portilexicon = pd.read_csv('portilexicon-ud - portilexicon-ud.csv')"
      ],
      "metadata": {
        "id": "C1AGEMYP1hD2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizacao_portilexicon(texto, portilexicon = portilexicon):\n",
        "    \"\"\"Lematiza um texto através do documento lexicográfico PortiLexicon.\n",
        "\n",
        "    Recebe:\n",
        "        texto (list): Lista de tokens (strings) do texto.\n",
        "        portilexicon (pd.DataFrame): DataFrame contendo as colunas 'a' (token) e 'a.1' (lemma).\n",
        "\n",
        "    Retorna:\n",
        "        list: Lista de tokens lematizados.\n",
        "    \"\"\"\n",
        "\n",
        "    # Criar um dicionário para busca rápida dos lemas\n",
        "    lemma_dict = dict(zip(portilexicon['a'], portilexicon['a.1']))\n",
        "\n",
        "    # Dá flat na list comprehension antes do processamento\n",
        "    flat_texto = [item for sublist in texto for item in sublist]\n",
        "\n",
        "    if isinstance(flat_texto[0], list):\n",
        "        flat_texto = [item for sublist in flat_texto for item in sublist]\n",
        "\n",
        "    # Substituir tokens pelo lema correspondente ou manter o token original\n",
        "    return [lemma_dict.get(token, token) for token in flat_texto]"
      ],
      "metadata": {
        "id": "GyFoTsNFcoBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "port_lem = lematizacao_portilexicon(riogrande)"
      ],
      "metadata": {
        "id": "M5faxnFXItLt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "port_lem2 = lematizacao_portilexicon(riogrande_sem_stopwords)"
      ],
      "metadata": {
        "id": "r9rpz3K0oP4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DataFrame\n",
        "port_lem_df = pd.DataFrame({'Texto': [item for sublist in riogrande for item in sublist]})\n",
        "\n",
        "# Aplicar lematização aos dados\n",
        "port_lem_df['Texto Lematizado'] = port_lem"
      ],
      "metadata": {
        "id": "2-wkqscRnmYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DataFrame\n",
        "port_lem_df2 = pd.DataFrame({'Texto': [item for sublist in riogrande_sem_stopwords for item in sublist]})\n",
        "\n",
        "# Aplicar lematização aos dados\n",
        "port_lem_df2['Texto Lematizado'] = port_lem2"
      ],
      "metadata": {
        "id": "dvVf4juaoOfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporta o dataframe da lematização em um arquivo .csv\n",
        "port_lem_df.to_csv(r\"simlematizacaoRI.csv\")\n",
        "port_lem_df2.to_csv(r\"simlematizacaoRI_sem_stopwords.csv\")"
      ],
      "metadata": {
        "id": "i17A_ep3oGYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validação"
      ],
      "metadata": {
        "id": "APLHsLiOKtdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testeriogrande = [riogrande[:1000]]\n",
        "testeriogrande"
      ],
      "metadata": {
        "id": "CXUS_J8dKxHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lematizacaomodelo = ['informante', 'de', 'ensino', 'superior', 'informante', '1', 'joão', 'carlos', 'sexo', 'masculino', 'idade', '28', 'ano', 'data', 'de', 'coleta', 'oral', '20', '11', '93', 'escrita', '21', '11', '93', 'e',\n",
        "'22', '11', '93', 'parte', 'oral', 'narrativa', 'de', 'experiência', 'pessoal', 'e', 'joão', 'ter', 'algum', 'história', 'que', 'ter', 'acontecer', 'contigo', 'que', 'tu', 'querer', 'eu', 'contar', 'i',\n",
        "'ter', 'um', 'recente', 'eh', 'pegar', 'o', 'lancha', 'aí', 'ir', 'eu', 'e', 'dois', 'amigo', 'meu', 'que', 'trabalhar', 'comigo', 'lá', 'o', 'edson', 'e', 'o', 'miguel', 'aí',\n",
        "'entrar', 'em', 'lancha', 'né', 'ter', 'não', 'saber', 'se', 'você', 'saber', 'como', 'ser', 'que', 'o', 'lancha', 'ter', 'quatro', 'um', 'fileira', 'de','quatro', 'cadeira','assim',\n",
        "'de', 'madeira', 'com', 'encosto', 'e', 'o', 'cadeira', 'ser', 'normal', 'aí', 'até', 'ir', 'entrar', 'conversar', 'aí','pedir', 'pra', 'dar', 'lugar', 'pra', 'outro', 'sentar', 'também', 'sentar', 'em', 'canto', 'o', 'outro', 'dois', 'sentar',\n",
        "'perto','de', 'eu', 'só', 'que', 'eu', 'não', 'ver', 'que', 'estar', 'sem', 'encosto', 'o', 'cadeira', 'eu', 'sentar', 'o','cadeira', 'ser', 'de', 'ser', 'imóvel', 'vir', 'pra', 'trás', 'cair', 'deitado', 'em', 'colo', 'de',\n",
        "'negrão', 'atrás', 'de', 'eu', 'riso', 'aquilo', 'ali', 'ficar', 'o', 'vale', 'de', 'ano', 'lá', 'em', 'em', 'narrativa', 'recontado', 'e', 'estar', 'joão',  'e', 'ter', 'algum', 'história', 'que', 'alguém',\n",
        "'ter', 'te', 'contar', 'e', 'que', 'tu', 'não', 'ter', 'participado', 'e', 'que', 'tu', 'saber', 'através', 'de', 'alguém', 'i', 'eh', 'eu', 'não', 'saber', 'se', 'chegar', 'a', 'ser', 'o', 'que', 'tu', 'estar', 'querer',\n",
        "'mas', 'isso', 'acontecer', 'sexta', 'feira', 'também', 'ser', 'mais', 'pra',  'mais', 'um', 'vale', 'datar', 'o', 'cara', 'chegar', 'estar', 'eh', 'almoçar', 'o', 'mãe', 'dele', 'ligar', 'pra', 'ele', 'assim', '\\x93está',\n",
        "'com', 'um', 'pé', 'de', 'cada','sapato','aí','o','o','colega','chegar','e','disse','\\x93ué','pé','de','cada','sapato','você','achar','que','eu','ser','louco','por','um','pé','de','sapato','um','pé','de',\n",
        "'cada','sapato','olhar','pra','baixo','um','sapato','com','um','lista','e','outro','sem','lista','riso','de','e','ir','trabalhar','com','um','pé','de','cada','sapato','pegar','em','pé','dele','lá','descrição',\n",
        "'de','local','e','tá','joão','eh','ter','algum','lug','um','lugar','ou','algum','coisa','que','tu','querer','eu','descrever','i','eu','poder','você','descrever','o','agência','lá','em','em','norte','onde',\n",
        "'eu','trabalhar','eh','logo','em','seguida','qur','tu','descer','do','lancha','ter','o','o','rua','de','frente','ali','álvaro','costa','o','agência','ser','bem','em','frente','ser','dois','piso','né','o',\n",
        "'setor','de','baixo','ser','onde','funcionar','o','o','caixa','de','gerência','e','em','cima','o','pessoal','mais','o','pessoal','de','de','aplicação','ser','um','um','agência','pequeno','mas','eh','bem',\n",
        "'bonito','até','relato','de','procedimento','e','tá','e','ter','algum','coisa','que','tu','querer','eu','ensinar','a','fazer','explicar','explicar','como','faz','algum','coisa','pode','ser','algum','coisa',\n",
        "'relacionado','com','o','você','serviço','algum','i','ser','ser','difícil','ir','dizer','você','ensinar','a','trabalhar','em','caixa','o','o','rapaz','que','fazer','compensação','por','exemplo','quando',\n",
        "'você','chegar','com','um','depósito','com','cheque','de','outro','banco','tu','tu','tem','que','carimbar','o','cheque','né','aí','em','final','de','dia','tu','tu','somar','ele','banco','por','banco','que',\n",
        "'ficar','só','pra','pra','compensação','eh','ser','coisa','entender','que','eu','poder','até','você','explicar','mas','não','relato','de','opinião','e','aí','tu','querer','dar','um','opinião','sobre',\n",
        "'algum','coisa','algum','assunto','i','eh','como','eu','trabalhar','em','banco','agora','que','falar','tudo','agora','sobre','o','sobre','banco','ir','você','dar','um','motivo','um','opinião','sobre',\n",
        "'não','sei','aplicação','eu','achar','que','pelo','menos','ser','de','se','esperar','como','o','taxa','estar','muito','alta','em','aplicação','dever','ter','um','mudança','em','plano','econômico','de',\n",
        "'de','país','pro','final','de','ano','até','estar','falar','lá','em','algum','pacote','algum','coisa','porque','a','taxa','estar','muito','al','muito','alta','muito','acima','de','inflação','o','inflação',\n",
        "'que','ele','divulgar','né','então','por','a','taxa','estar','tão','alta','certamente','dever','ter','algum','mudança','em','em','plano','econômico','e','tá','i','bom','dia','e','obrigado','parte','escrito',\n",
        "'narrativa','de','experiência','pessoal','ir','descrever','um','acontecimento','engraçado','que','se','passar','comigo','em','lancha','para','são','josé','do','norte','quando','ia','para','o','serviço',\n",
        "'a','lancha','noivo','de','mar','ter','fileira','com','4','banco','de','madeira','e','o','assento','ser','móvel','entrar','em','lancha','juntamente','com','dois','colega','de','serviço','e','sentei','em',\n",
        "'cadeira','de','canto','porém','não','ver','que','o','cadeira','estar','sem','encosto','cair','com','tudo','pra','trás','em','colo','de','um','senhor','que','estar','no','banco','de','trás','ser','o',\n",
        "'vale','do','ano','narrativa','recontado','outro','acontecimento','engraçado','acontecer','outro','dia','com','um','colega','em','serviço','o','mãe','dele','ligar','pra','ele','perguntar','se','ele','estar',\n",
        "'com','um','pé','de','cada','sapato','ele','ainda','ficar','bravo','dizer','que','não','estar','louco','e','olhar','para','o','pé', 'estar','mesmo','com','um','pé','de','cada','imaginar','o','situação',\n",
        "'descrição','de','local','ir','descrever','o','agência','de','caixa','econômico','federal','em','são','josé','do','norte','ser','logo','em','rua','em','frente','o','hidroviário','um','agência','de','dois',\n",
        "'andar','onde','em','parte','inferior','ficar','localizado','o','gerência','o','abertura','de','conta','e','o','caixa','em','andar','superior','ficar','o','cozinha','e','o','banheiro','o','almoxarifado',\n",
        "'e','o','setor','de','contabilidade','e','aplicação','ser','um','agência','pequeno','porém','bem','agradável','relato','de','procedimento','ir','dar','um','pequeno','procedimento','sobre','o','tratamento',\n",
        "'dado','a','cheque','de','outro','banco','recebido','pelo','caixa','em','depósito','o','caixa','recebe','o','depósito','em','cheque','carimbar','o','cheque','com','o','carimbo','de','compensação','em',\n",
        "'verso','e','com','o','de','cruzamento','em','frente','verificar','o','prazo','de','comp','bloqueio','e','autenticar','o','depósito','em','final','de','expediente','fazer','o','soma','de','cheque','verificar',\n",
        "'se','coincidir','com','o','total','de','depósito','recebido','e','encaminhar','para','o', 'retaguarda','relato', 'de', 'opinião', 'ir', 'dar','um', 'opinião','sobre','o','situação', 'econômico', 'que',\n",
        "'provavelmente', 'dever', 'ter', 'mudança', 'em', 'final', 'desse', 'ano', 'visto', 'que', 'o', 'taxa', 'de', 'juro','em', 'aplicação', 'em', 'cdb', 'rdb', 'poupança', 'e','outro','estar','muito','elevado',\n",
        "'em','relação','o', 'inflação', 'divulgado', 'achar', 'que', 'certamente', 'esse', 'equilíbrio', 'provocar', 'mudança', 'e', 'quem', 'saber', 'um', 'novo','pacote', 'econômico', 'informante', '2',\n",
        "'lisandra','sexo','feminino','idade', '23','ano','data','de','coleta','oral','28','09','93','escrita','06','11','93','parte','escrito','narrativa','de','experiência','pessoal','e','lisandra', 'um',\n",
        "'experiência', 'um', 'narrativa', 'teu', 'de', 'experiência']"
      ],
      "metadata": {
        "id": "cFBaFu3oK5W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contaacuracia(lematizacaomodelo, lematizacao):\n",
        "\n",
        "  corretos = 0\n",
        "\n",
        "  for i in range(len(lematizacaomodelo)):\n",
        "    if lematizacaomodelo[i] == lematizacao[i]:\n",
        "      corretos += 1\n",
        "\n",
        "  acuracia = corretos / len(lematizacaomodelo)\n",
        "\n",
        "  return acuracia"
      ],
      "metadata": {
        "id": "3oopRJzVcER3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contaacuracia(lematizacaomodelo, spacy_lem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2dmMU-LcXa7",
        "outputId": "0cc233b4-9ab8-469a-b3e6-0c34efb6dda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.802"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contaacuracia(lematizacaomodelo, sim_lematizacao)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSLF9SnDc4pF",
        "outputId": "3ac13692-0aba-455d-f85c-9ac2767ee5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.825"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contaacuracia(lematizacaomodelo, port_lem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-d1a37Rc9IZ",
        "outputId": "22310f54-eeaf-49a3-9a1e-8c14c93e1b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.802"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "e6kVWAx8ixeZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXX/0nXxkJg6Ng9+okAJWR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}